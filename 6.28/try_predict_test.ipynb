{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42f9e3e5-675b-44b0-92f3-da6ba687e81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Open       High        Low      Close  Adj Close   Volume\n",
      "Date                                                                      \n",
      "2022-01-03  41.950001  42.349998  41.900002  42.150002  39.278328  1761872\n",
      "2022-01-04  42.099998  42.349998  41.500000  42.299999  39.418106  2910078\n",
      "2022-01-05  42.349998  42.799999  42.250000  42.700001  39.790859  4416855\n",
      "2022-01-06  42.750000  42.799999  42.000000  42.349998  39.464703  1457827\n",
      "2022-01-07  42.250000  42.500000  41.900002  42.500000  39.604481  2066317\n",
      "...               ...        ...        ...        ...        ...      ...\n",
      "2024-03-22  25.900000  25.950001  25.299999  25.450001  25.450001  3367528\n",
      "2024-03-25  25.500000  25.750000  25.250000  25.600000  25.600000  2579997\n",
      "2024-03-26  26.000000  26.299999  25.600000  26.150000  26.150000  5201511\n",
      "2024-03-27  26.100000  26.350000  25.850000  26.150000  26.150000  4277108\n",
      "2024-03-28  26.200001  26.299999  25.700001  25.799999  25.799999  3276381\n",
      "\n",
      "[550 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pandas_datareader import data\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from models.LSTM import LSTM\n",
    "#from utils.split_data import split_data\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from torch.nn import GRU\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from itertools import product\n",
    "\n",
    "def split_data(stock, lookback):\n",
    "    data_raw = stock\n",
    "    data = []\n",
    "    for index in range(len(data_raw) - lookback):\n",
    "        data.append(data_raw[index: index + lookback])\n",
    "        \n",
    "    data = np.array(data)\n",
    "    test_set_size = int(np.round(0.2*stock.shape[0]))\n",
    "    train_set_size = stock.shape[0] - test_set_size\n",
    "\n",
    "    x_train = data[:train_set_size, :-1,:]\n",
    "    y_train = data[:train_set_size, -1, :]\n",
    "\n",
    "    x_test = data[train_set_size:, :-1, :]\n",
    "    y_test = data[train_set_size:, -1, :]\n",
    "    \n",
    "    return (x_train, y_train, x_test, y_test)\n",
    "\n",
    "datapath = 'stock_data/0066.HK.csv'\n",
    "startdate = \"2022-01-01\"\n",
    "enddate = \"2024-3-30\"\n",
    "\n",
    " #####读取数据\n",
    "data_to_be_predict = pd.read_csv(datapath)\n",
    "data_to_be_predict['Date'] = pd.to_datetime(data_to_be_predict['Date'])\n",
    "data_to_be_predict = data_to_be_predict.set_index('Date')\n",
    "data_to_be_predict = data_to_be_predict.loc[startdate:enddate]\n",
    "\n",
    "yf.pdr_override()\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))  \n",
    "\n",
    "print(data_to_be_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69832065-e047-497b-b256-d7641d4a6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    which_model = 'lstm'\n",
    "\n",
    "    lookback = 50\n",
    "    # Open = data_to_be_predict['Open'].values.reshape(-1,1)\n",
    "    data_close = scaler.fit_transform(data_to_be_predict['Close'].values.reshape(-1,1))\n",
    "                \n",
    "    x_train, y_train, x_test, y_test = split_data(data_close, lookback)\n",
    "\n",
    "    x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "    y_test_lstm = torch.from_numpy(y_test).type(torch.Tensor)\n",
    "    x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "    y_train_lstm = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "\n",
    "    model = torch.load('pthfile/lstm_model_complete.pth')\n",
    "\n",
    "    data_close = scaler.inverse_transform(data_close)\n",
    "\n",
    "    test_pred = model(x_test)\n",
    "    test_pred = pd.DataFrame((test_pred.detach().numpy()))\n",
    "    train_pred = model(x_train)\n",
    "    train_pred = pd.DataFrame((train_pred.detach().numpy()))\n",
    "\n",
    "    trainPredictPlot=np.empty_like(data_close)\n",
    "    trainPredictPlot[:,:]=np.nan\n",
    "    trainPredictPlot[lookback:len(scaler.inverse_transform(train_pred))+lookback] = scaler.inverse_transform(train_pred)\n",
    "\n",
    "    testPredictPlot=np.empty_like(data_close)\n",
    "    testPredictPlot[:,:] = np.nan\n",
    "    testPredictPlot[len(train_pred)+(lookback):len(data_close)] = scaler.inverse_transform(test_pred)\n",
    "\n",
    "    ####整体predict序列\n",
    "    whole_predict = np.empty_like(data_close)\n",
    "    whole_predict[:,:] = np.nan\n",
    "    whole_predict[lookback:len(scaler.inverse_transform(train_pred))+lookback] = scaler.inverse_transform(train_pred)\n",
    "    whole_predict[len(train_pred)+(lookback):len(data_close)] = scaler.inverse_transform(test_pred)\n",
    "    # actual = data_close[lookback:]\n",
    "\n",
    "    whole_predict = pd.DataFrame({'Date':data_to_be_predict.index,'pred_close':whole_predict.flatten(),'act_close':data_to_be_predict['Close'],'open':data_to_be_predict['Open']})\n",
    "\n",
    "    trainPredictPlot = pd.DataFrame({'Date':data_to_be_predict.index,'Close':trainPredictPlot.flatten()})\n",
    "    testPredictPlot = pd.DataFrame({'Date':data_to_be_predict.index,'Close':testPredictPlot.flatten()})\n",
    "    data_close = pd.DataFrame({'Date':data_to_be_predict.index,'Close':data_close.flatten()})\n",
    "\n",
    "    mse_test = mean_squared_error(y_test_lstm.detach().numpy(),test_pred)\n",
    "    mse_train = mean_squared_error(y_train_lstm.detach().numpy(),train_pred)\n",
    "\n",
    "    #return mse_train,mse_test,testPredictPlot,trainPredictPlot,data_close,whole_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e00bb44-4ecf-4459-bb87-20057e99d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    which_model = 'linearregression'\n",
    "\n",
    "    # stock_data = data.get_data_yahoo('0066.HK', start=\"2010-01-01\", end=\"2020-06-30\").reset_index()\n",
    "    data_to_be_predict_shift = data_to_be_predict.shift(1)\n",
    "    data_to_be_predict_shift = data_to_be_predict_shift.iloc[1:]\n",
    "    data_to_be_predict = data_to_be_predict.iloc[1:]\n",
    "    predict_x = data_to_be_predict_shift[['Open', 'High', 'Low', 'Volume','Close']]\n",
    "    predict_y = data_to_be_predict['Close']\n",
    "\n",
    "    # print(predict_x)\n",
    "    predict_y = predict_y.sort_index()\n",
    "    predict_x = predict_x.sort_index()\n",
    "    model = joblib.load('pthfile/linear_regression_model.joblib')\n",
    "    predict_result = model.predict(predict_x)\n",
    "    mse = mean_squared_error(predict_y, predict_result)\n",
    "\n",
    "    pred = pd.DataFrame({'Date':pd.to_datetime(predict_x.index),'Close':predict_result})\n",
    "    actual = pd.DataFrame({'Date':pd.to_datetime(predict_x.index),'Close':predict_y.values})\n",
    "\n",
    "    whole_predict = pd.DataFrame({'Date':data_to_be_predict.index,'pred_close':predict_result.flatten(),'open':data_to_be_predict['Open'],'act_close':data_to_be_predict['Close']})\n",
    "    ###trading 在开盘时，如果预测今天的close比开盘价高则买入\n",
    "\n",
    "    #return mse,actual,pred,whole_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "673453bd-57fc-40ed-9d79-d47dfa984d4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (600,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     y_test\u001b[38;5;241m.\u001b[39mappend(mtr[i])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 转换为 numpy 数组并重塑为二维数组\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m x_train, y_train, x_test, y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x_train), np\u001b[38;5;241m.\u001b[39marray(y_train), np\u001b[38;5;241m.\u001b[39marray(x_test), np\u001b[38;5;241m.\u001b[39marray(y_test)\n\u001b[0;32m     19\u001b[0m x_train \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39mreshape(x_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m x_test \u001b[38;5;241m=\u001b[39m x_test\u001b[38;5;241m.\u001b[39mreshape(x_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (600,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "\n",
    "    which_model = 'randomforest'\n",
    "   \n",
    "    # stock_data = data.get_data_yahoo('0066.HK', start=\"2010-01-01\", end=\"2020-06-30\").reset_index()\n",
    "    lookback = 600               \n",
    "        \n",
    "    mtr = scaler.fit_transform(data_to_be_predict.values)\n",
    "    # 构建训练集和测试集\n",
    "    x_train, y_train, x_test, y_test = [], [], [], []\n",
    "    for i in range(len(mtr) - lookback):\n",
    "        x_train.append(mtr[i:i+lookback])\n",
    "        y_train.append(mtr[i+lookback])\n",
    "\n",
    "    for i in range(len(mtr) - lookback, len(mtr)):\n",
    "        x_test.append(mtr[i-lookback:i])\n",
    "        y_test.append(mtr[i])\n",
    "\n",
    "    # 转换为 numpy 数组并重塑为二维数组\n",
    "    x_train, y_train, x_test, y_test = np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)\n",
    "    x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "    # 构建训练集和测试集\n",
    "   \n",
    "    # Load pre-trained Random Forest model\n",
    "    model = torch.load('pthfile/randomforest_model.pth')\n",
    "        \n",
    "    #model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    #model.fit(x_train, y_train.ravel())\n",
    "\n",
    "    train_pred = model.predict(x_train)\n",
    "    test_pred = model.predict(x_test)\n",
    "        \n",
    "    mse_train = mean_squared_error(y_train, train_pred)\n",
    "    mse_test = mean_squared_error(y_test, test_pred)\n",
    "        \n",
    "    whole_predict = pd.DataFrame({'Date':data_to_be_predict.index, 'pred_close': np.concatenate([train_pred, test_pred]), 'open':data_to_be_predict['Open'],'act_close':data_to_be_predict['Close']})\n",
    "    \n",
    "    #return mse_train, mse_test, y_train, train_pred, y_test, test_pred, mtr, whole_predict\n",
    "    #return mse_train, mse_test, y_train, train_pred, y_test, test_pred, mtr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "409655cc-cc8d-451e-8700-864bcf8bb55d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (600,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m x_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x_train)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, lookback, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_train)\n\u001b[1;32m---> 18\u001b[0m x_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x_test)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, lookback, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_test)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Load pre-trained Random Forest model\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (600,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "lookback = 600\n",
    "\n",
    "mtr = scaler.fit_transform(data_to_be_predict.values)\n",
    "\n",
    "# 构建训练集和测试集\n",
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "for i in range(len(mtr) - lookback):\n",
    "    x_train.append(mtr[i:i+lookback])\n",
    "    y_train.append(mtr[i+lookback])\n",
    "\n",
    "for i in range(len(mtr) - lookback, len(mtr)):\n",
    "    x_test.append(mtr[i-lookback:i])\n",
    "    y_test.append(mtr[i])\n",
    "\n",
    "x_train = np.vstack(x_train)\n",
    "y_train = np.vstack(y_train)\n",
    "x_test = np.vstack(x_test)\n",
    "y_test = np.vstack(y_test)\n",
    "\n",
    "# 转换为 numpy 数组并重塑为三维数组\n",
    "x_train = np.array(x_train).reshape(-1, lookback, 1)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test).reshape(-1, lookback, 1)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Load pre-trained Random Forest model\n",
    "model = torch.load('pthfile/randomforest_model.pth')\n",
    "\n",
    "# 进行预测\n",
    "train_pred = model.predict(x_train)\n",
    "test_pred = model.predict(x_test)\n",
    "\n",
    "mse_train = mean_squared_error(y_train, train_pred)\n",
    "mse_test = mean_squared_error(y_test, test_pred)\n",
    "\n",
    "# 构建整体预测的 DataFrame\n",
    "whole_predict = pd.DataFrame({'Date': data_to_be_predict.index,\n",
    "                              'pred_close': np.concatenate([train_pred, test_pred]),\n",
    "                              'open': data_to_be_predict['Open'],\n",
    "                              'act_close': data_to_be_predict['Close']})\n",
    "\n",
    "mse_train, mse_test, y_train, train_pred, y_test, test_pred, mtr, whole_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d21accf4-b983-4b28-aba0-86602f09e28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patty.hao\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SVR from version 1.0.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 600 features, but SVR is expecting 100 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpthfile/svm_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 进行预测\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m train_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_train)\n\u001b[0;32m     25\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 计算均方误差\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\svm\\_base.py:428\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    413\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \n\u001b[0;32m    415\u001b[0m \u001b[38;5;124;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 428\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_for_predict(X)\n\u001b[0;32m    429\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\svm\\_base.py:606\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    603\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[1;32m--> 606\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    607\u001b[0m         X,\n\u001b[0;32m    608\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    609\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[0;32m    610\u001b[0m         order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    611\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    612\u001b[0m         reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    613\u001b[0m     )\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m    616\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 600 features, but SVR is expecting 100 features as input."
     ]
    }
   ],
   "source": [
    "\n",
    "    which_model = 'svm'\n",
    "    \n",
    "    lookback = 100   \n",
    "        \n",
    "    mtr = scaler.fit_transform(data_to_be_predict.values)\n",
    "    # 构建训练集和测试集\n",
    "    x_train, y_train, x_test, y_test = [], [], [], []\n",
    "    for i in range(len(mtr) - lookback):\n",
    "        x_train.append(mtr[i:i+lookback])\n",
    "        y_train.append(mtr[i+lookback])\n",
    "    for j in range(len(mtr) - lookback, len(mtr)):\n",
    "        x_test.append(mtr[j-lookback:j])\n",
    "        y_test.append(mtr[j])\n",
    "\n",
    "    # 转换为 numpy 数组并重塑为二维数组\n",
    "    x_train, y_train, x_test, y_test =  np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)\n",
    "    x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "    # Load pre-trained SVM model\n",
    "    model = torch.load('pthfile/svm_model.pth')\n",
    "\n",
    "    # 进行预测\n",
    "    train_pred = model.predict(x_train)\n",
    "    test_pred = model.predict(x_test)\n",
    "        \n",
    "    # 计算均方误差\n",
    "    mse_train = mean_squared_error(y_train, train_pred)\n",
    "    mse_test = mean_squared_error(y_test, test_pred)\n",
    "\n",
    "    # 绘制预测结果\n",
    "    plt.plot(y_train, label='Actual Train')\n",
    "    plt.plot(train_pred, label='Predicted Train')\n",
    "    plt.legend()\n",
    "    plt.title('SVM Predictions on Train Data')\n",
    "    plt.show()\n",
    "        \n",
    "    plt.plot(y_test, label='Actual Test')\n",
    "    plt.plot(test_pred, label='Predicted Test')\n",
    "    plt.legend()\n",
    "    plt.title('SVM Predictions on Test Data')\n",
    "    plt.show()\n",
    "        \n",
    "    whole_predict = pd.DataFrame({'Date':data_to_be_predict.index, 'pred_close': np.concatenate([train_pred, test_pred]), 'open':data_to_be_predict['Open'],'act_close':data_to_be_predict['Close']})\n",
    "\n",
    "    #return mse_train, mse_test, y_train, train_pred, y_test, test_pred, mtr, whole_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6e2e210-452f-4465-bbbe-3380076bd4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gru(datapath = 'stock_data/0066.HK.csv',startdate = \"2022-01-01\",enddate = \"2024-3-30\"):\n",
    "    which_model == 'gru'\n",
    "    #####读取数据\n",
    "    data_to_be_predict = pd.read_csv(datapath)\n",
    "    data_to_be_predict['Date'] = pd.to_datetime(data_to_be_predict['Date'])\n",
    "    data_to_be_predict = data_to_be_predict.set_index('Date')\n",
    "    data_to_be_predict = data_to_be_predict.loc[startdate:enddate]\n",
    "\n",
    "    yf.pdr_override()\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))  \n",
    "    # stock_data = data.get_data_yahoo('0066.HK', start=\"2010-01-01\", end=\"2020-06-30\").reset_index()\n",
    "    # 数据准备\n",
    "    def prepare_data(data, lookback):\n",
    "        x, y = [], []\n",
    "        for i in range(len(data) - lookback):\n",
    "            x.append(data[i:(i + lookback)])\n",
    "            y.append(data[i + lookback])\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    # 加载数据\n",
    "    csv_file_path = 'SSE_Index.csv'\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # 数据预处理\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_data = scaler.fit_transform(df.values)\n",
    "\n",
    "    # 定义超参数\n",
    "    lookback = 100\n",
    "\n",
    "    # 加载预训练的GRU模型\n",
    "    loaded_model = torch.jit.load('pthfile/gru_model_script.pth')\n",
    "\n",
    "    # 准备测试集\n",
    "    train_data_scaled = scaler.transform(df.values)\n",
    "    x_train, y_train = prepare_data(train_data_scaled, lookback)\n",
    "    x_train = torch.from_numpy(x_train).type(torch.Tensor)  # 将 ndarray 转换为 Tensor\n",
    "        \n",
    "    test_data_scaled = scaler.transform(df.values)\n",
    "    x_test, y_test = prepare_data(test_data_scaled, lookback)\n",
    "    x_test = torch.from_numpy(x_test).type(torch.Tensor)  # 将 ndarray 转换为 Tensor\n",
    "\n",
    "    # 使用加载的模型进行预测\n",
    "    loaded_model.eval()\n",
    "    train_pred = loaded_model(x_train)\n",
    "    train_pred = scaler.inverse_transform(train_pred.detach().numpy())\n",
    "    test_pred = loaded_model(x_test)\n",
    "    test_pred = scaler.inverse_transform(test_pred.detach().numpy())\n",
    "\n",
    "    # 可视化预测结果\n",
    "    # 绘制训练集预测结果\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df.index[lookback:len(train_pred) + lookback], df['Close'][lookback:len(train_pred) + lookback], label='Actual Train')\n",
    "    plt.plot(df.index[lookback:len(train_pred) + lookback], train_pred, label='Predicted Train')\n",
    "    plt.legend()\n",
    "    plt.title('GRU Predictions on Train Data')\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df.index[lookback:], df['Close'][lookback:], label='Actual Test')\n",
    "    plt.plot(df.index[lookback:], test_pred, label='Predicted Test (GRU Model)')\n",
    "    plt.legend()\n",
    "    plt.title('GRU Predictions on Test Data')\n",
    "    plt.show()\n",
    "\n",
    "    # 计算均方误差\n",
    "    mse_train = mean_squared_error(y_train, train_pred)\n",
    "    mse_test = mean_squared_error(y_test, test_pred)#df['Close'][lookback:]\n",
    "    # print('MSE ON TRAIN:', mse_train)\n",
    "    # print('MSE ON TEST:', mse_test) \n",
    "        \n",
    "    whole_predict = pd.DataFrame({'Date':data_to_be_predict.index, 'pred_close': np.concatenate([train_pred, test_pred]), 'open':data_to_be_predict['Open'],'act_close':data_to_be_predict['Close']})\n",
    "\n",
    "    return mse_train, mse_test, y_train, train_pred, y_test, test_pred, scaled_data, whole_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b71e39-3404-46c0-a4d3-7df6667cc285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
